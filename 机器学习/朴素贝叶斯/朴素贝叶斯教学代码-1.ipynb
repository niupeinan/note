{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>朴素贝叶斯代码</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 恶意留言过滤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 原理出发"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.载入训练数据集  2.创建词集（去重）3.把训练集中文本转成向量形式（词集模型）4.训练：计算先验概率和先验条件概率（即频率），为防止下溢，取对数 5.分类 6.测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.091042453358316 -3.9512437185814275\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def loadData():\n",
    "    wordList=[['my','name','is','David'],\n",
    "                 ['you','are','stupid'],\n",
    "                 ['my','boyfriend','is','SB'],\n",
    "                 ['you','looks','very','smart','I','like','you','very','much']]\n",
    "    classList=[0,1,1,0]\n",
    "    return wordList,classList\n",
    "\n",
    "def creatVocabList(wordList):\n",
    "    vocabSet=set([])\n",
    "    for document in wordList:\n",
    "        vocabSet=vocabSet|set(document)\n",
    "    vocabList=list(vocabSet)    \n",
    "    return vocabList\n",
    "\n",
    "def setOfWords2Vec(vocabList,words):\n",
    "    wordVec=np.zeros(len(vocabList))\n",
    "    for word in words:\n",
    "        if word in vocabList:\n",
    "            wordVec[vocabList.index(word)]=1\n",
    "    return wordVec\n",
    "\n",
    "wordList,classList=loadData()\n",
    "vocabList=creatVocabList(wordList)\n",
    "\n",
    "trainMat=[]\n",
    "for words in wordList:\n",
    "    trainMat.append(setOfWords2Vec(vocabList,words))\n",
    "\n",
    "def trainNB(trainMat,classList):\n",
    "    numWords=len(vocabList)\n",
    "    pSpam=(sum(classList)+1)/(len(classList)+2)\n",
    "    p1Num=np.ones(numWords)\n",
    "    p0Num=np.ones(numWords)\n",
    "    p0Denom=0\n",
    "    p1Denom=0\n",
    "    for i in range(len(classList)):\n",
    "        if classList[i]==1:\n",
    "            p1Num+=trainMat[i]\n",
    "            p1Denom+=sum(trainMat[i])\n",
    "        else:\n",
    "            p0Num+=trainMat[i]\n",
    "            p0Denom+=sum(trainMat[i])\n",
    "    p1Denom+=numWords      \n",
    "    p0Denom+=numWords \n",
    "    p1Vec=p1Num/p1Denom\n",
    "    p0Vec=p0Num/p0Denom\n",
    "    return p1Vec,p0Vec,pSpam\n",
    "\n",
    "def classifyNB(newWordVec):\n",
    "    p1Vec,p0Vec,pSpam=trainNB(trainMat,classList)\n",
    "    a=1\n",
    "    b=1\n",
    "    for i in range(len(vocabList)):\n",
    "        if (newWordVec*p1Vec)[i]!=0:\n",
    "            a*=(newWordVec*p1Vec)[i]\n",
    "    for i in range(len(vocabList)):\n",
    "        if (newWordVec*p0Vec)[i]!=0:\n",
    "            b*=(newWordVec*p0Vec)[i]\n",
    "    p1=np.log(a)+np.log(pSpam)\n",
    "    p0=np.log(b)+np.log((1-pSpam))\n",
    "    print(p1,p0)\n",
    "    if p1>p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "testWords=['stupid']   \n",
    "newWordVec=setOfWords2Vec(vocabList,testWords)\n",
    "print(classifyNB(newWordVec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果是词集模型，等价于直接用以下代码（简单化了）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.044522437723423 -3.9512437185814275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'粗鲁'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loadData():\n",
    "    wordList=[['my','name','is','David'],\n",
    "                 ['you','are','stupid'],\n",
    "                 ['my','boyfriend','is','NB'],\n",
    "                 ['you','looks','very','smart','I','like','you','very','much']]\n",
    "    classList=[0,1,1,0]\n",
    "    return wordList,classList\n",
    "\n",
    "wordList,classList=loadData()\n",
    "\n",
    "def creatVocabList(wordList):\n",
    "    vocabSet=set([])\n",
    "    for document in wordList:\n",
    "        vocabSet=vocabSet|set(document)\n",
    "    vocabList=list(vocabSet)\n",
    "    return vocabList\n",
    "\n",
    "def setOfWord2Vec(vocabList,words):\n",
    "    wordVec=np.zeros(len(vocabList))\n",
    "    for word in words:\n",
    "        if word in vocabList:\n",
    "            wordVec[vocabList.index(word)]=1\n",
    "    return wordVec\n",
    "\n",
    "\n",
    "trainMat=[]\n",
    "for words in wordList:\n",
    "    trainMat.append(setOfWord2Vec(vocabList,words))\n",
    "    \n",
    "def trainNB(trainMat,classList):\n",
    "    numWords=len(vocabList)\n",
    "    pSpam=(sum(classList)+1)/(len(classList)+2)\n",
    "    p1Num=np.ones(numWords)\n",
    "    p0Num=np.ones(numWords)\n",
    "    p1Denom=0\n",
    "    p0Denom=0\n",
    "    for i in range((len(classList))):\n",
    "        if classList[i]==1:\n",
    "            p1Num+=trainMat[i]\n",
    "            p1Denom+=sum(trainMat[i])\n",
    "        else:\n",
    "            p0Num+=trainMat[i]\n",
    "            p0Denom+=sum(trainMat[i])\n",
    "    p1Denom+=numWords\n",
    "    p0Denom+=numWords\n",
    "    p1Vec=np.log(p1Num/p1Denom)\n",
    "    p0Vec=np.log(p0Num/p0Denom)\n",
    "    return p1Vec,p0Vec,pSpam\n",
    "\n",
    "vocabList=creatVocabList(wordList)\n",
    "def classifyNB(newWordVec,classList):\n",
    "    p1Vec,p0Vec,pSpam=trainNB(trainMat,classList)\n",
    "    p1=sum(p1Vec*newWordVec)+np.log(pSpam)\n",
    "    p0=sum(p0Vec*newWordVec)+np.log(1-pSpam)\n",
    "    print(p1,p0)\n",
    "    if p1>p0:\n",
    "        return '粗鲁'\n",
    "    else:\n",
    "        return '文明'\n",
    "    \n",
    "testWords=['stupid']\n",
    "newWordVec=setOfWord2Vec(vocabList,testWords)\n",
    "classifyNB(newWordVec,classList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn实现恶意留言过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
    "def loadData():\n",
    "    wordList=[['my','name','is','David'],\n",
    "                 ['you','are','stupid'],\n",
    "                 ['my','boyfriend','is','SB'],\n",
    "                 ['you','looks','very','smart','I','like','you','very','much']]\n",
    "    classList=[0,1,1,0]\n",
    "    return wordList,classList\n",
    "\n",
    "def creatVocabList(wordList):\n",
    "    vocabSet=set([])\n",
    "    for document in wordList:\n",
    "        vocabSet=vocabSet|set(document)\n",
    "    vocabList=list(vocabSet)    \n",
    "    return vocabList\n",
    "\n",
    "def setOfWords2Vec(vocabList,words):\n",
    "    wordVec=np.zeros(len(vocabList))\n",
    "    for word in words:\n",
    "        if word in vocabList:\n",
    "            wordVec[vocabList.index(word)]=1\n",
    "    return wordVec\n",
    "\n",
    "wordList,classList=loadData()\n",
    "vocabList=creatVocabList(wordList)\n",
    "\n",
    "trainMat=[]\n",
    "for words in wordList:\n",
    "    trainMat.append(setOfWords2Vec(vocabList,words))\n",
    "    \n",
    "ml=MultinomialNB()\n",
    "ml=ml.fit(trainMat,classList)\n",
    "testWords=['I','like','you']   \n",
    "newWordVec=setOfWords2Vec(vocabList,testWords)\n",
    "ml.predict_proba(np.array(newWordVec).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 垃圾邮件分类:英文版"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 原理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-18.392281269430665 -18.450059801885526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "def creatVocabList(wordList):\n",
    "    vocabSet=set([])\n",
    "    for document in wordList:\n",
    "        vocabSet=vocabSet|set(document)\n",
    "    vocabList=list(vocabSet)\n",
    "    return vocabList\n",
    "\n",
    "def setOfWords2Vec(vocabList,words):\n",
    "    wordVec=[0]*len(vocabList)\n",
    "    for word in words:\n",
    "        if word in vocabList:\n",
    "            wordVec[vocabList.index(word)]=1        \n",
    "    return wordVec\n",
    "\n",
    "def bagOfWords2Vec(vocabList,words):\n",
    "    wordVec=[0]*len(vocabList)\n",
    "    for word in words:\n",
    "        if word in vocabList:\n",
    "            wordVec[vocabList.index(word)]+=1        \n",
    "    return wordVec\n",
    "\n",
    "def trainNB(trainMat,classList):\n",
    "    numWords=len(vocabList)\n",
    "    pSpam=(sum(classList)+1)/(len(classList)+2)\n",
    "    p1Num=np.ones(numWords)\n",
    "    p0Num=np.ones(numWords)\n",
    "    p0Denom=0\n",
    "    p1Denom=0\n",
    "    for i in range(len(classList)):\n",
    "        if classList[i]==1:\n",
    "            p1Num+=trainMat[i]\n",
    "            p1Denom+=sum(trainMat[i])\n",
    "        else:\n",
    "            p0Num+=trainMat[i]\n",
    "            p0Denom+=sum(trainMat[i])\n",
    "    p1Denom+=numWords      \n",
    "    p0Denom+=numWords \n",
    "    p1Vec=np.log(p1Num/p1Denom)\n",
    "    p0Vec=np.log(p0Num/p0Denom)\n",
    "    return p1Vec,p0Vec,pSpam\n",
    "\n",
    "def classifyNB(newWordVec):\n",
    "    p1Vec,p0Vec,pSpam=trainNB(trainMat,classList)\n",
    "    p1=sum(p1Vec*newWordVec)+np.log(pSpam)\n",
    "    p0=sum(p0Vec*newWordVec)+np.log(1-pSpam)\n",
    "    print(p1,p0)\n",
    "    if p1>p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def textParse(bigString):\n",
    "    line=re.split('\\W*',bigString)\n",
    "    return [tokens.lower() for tokens in line if len(tokens)>2]   \n",
    "\n",
    "for i in range(1,26):\n",
    "    wordList=[];classList=[]\n",
    "    wordList_s=textParse(open('D:\\\\Downloads\\\\Naive_Bayes-master\\\\email\\\\spam\\\\%d.txt'%i).read())\n",
    "    wordList.append(wordList_s)\n",
    "    classList.append(1)\n",
    "    \n",
    "    wordList_h=textParse(open('D:\\\\Downloads\\\\Naive_Bayes-master\\\\email\\\\health\\\\%d.txt'%i).read())\n",
    "    wordList.append(wordList_h)\n",
    "    classList.append(0)\n",
    "    \n",
    "vocabList=creatVocabList(wordList)\n",
    "trainMat=[]\n",
    "for words in wordList:\n",
    "    trainMat.append(setOfWords2Vec(vocabList,words))\n",
    "\n",
    "def predict(path):\n",
    "    testWords=textParse(open(path).read())\n",
    "    newWordsVec=setOfWords2Vec(vocabList,testWords)\n",
    "    p1Vec,p0Vec,pSpam=trainNB(trainMat,classList)\n",
    "    return classifyNB(newWordsVec)\n",
    "    \n",
    "path='D:\\\\Downloads\\\\Naive_Bayes-master\\\\email\\\\spam\\\\3.txt'\n",
    "predict(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn实现英文垃圾邮件分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def creatVocabList(wordList):\n",
    "    vocabSet=set([])\n",
    "    for document in wordList:\n",
    "        vocabSet=vocabSet|set(document)\n",
    "    vocabList=list(vocabSet)    \n",
    "    return vocabList\n",
    "\n",
    "def setOfWords2Vec(vocabList,words):\n",
    "    wordVec=np.zeros(len(vocabList))\n",
    "    for word in words:\n",
    "        if word in vocabList:\n",
    "            wordVec[vocabList.index(word)]=1\n",
    "    return wordVec\n",
    "\n",
    "def textParse(bigString):\n",
    "    line=re.split('\\W*',bigString)#\\W+\n",
    "    return [tokens.lower() for tokens in line if len(tokens)>2]\n",
    "\n",
    "wordList=[]\n",
    "classList=[]\n",
    "for i in range(1,26):\n",
    "    wordList_s=textParse(open('D:\\\\Downloads\\\\Naive_Bayes-master\\\\email\\\\spam\\\\%d.txt'%i).read())\n",
    "    wordList.append(wordList_s)\n",
    "    classList.append(1)\n",
    "    \n",
    "    wordList_h=textParse(open('D:\\\\Downloads\\\\Naive_Bayes-master\\\\email\\\\health\\\\%d.txt'%i).read())\n",
    "    wordList.append(wordList_h)\n",
    "    classList.append(0)\n",
    "    \n",
    "trainMat=[]\n",
    "for words in wordList:\n",
    "    trainMat.append(setOfWords2Vec(vocabList,words))\n",
    "    \n",
    "gn=GaussianNB()\n",
    "gn=gn.fit(trainMat,classList)\n",
    "testWords=textParse(open('D:\\\\Downloads\\\\Naive_Bayes-master\\\\email\\\\health\\\\4.txt').read())\n",
    "newWordVec=setOfWords2Vec(vocabList,testWords)\n",
    "gn.predict(newWordVec.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改进版：预测新邮件：中文版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 3.417 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-361.7496110786985 -447.27585992956904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'粗鲁'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def creatVocabList(wordList):\n",
    "    vocabSet=set([])\n",
    "    for document in wordList:\n",
    "        vocabSet=vocabSet|set(document)\n",
    "    vocabList=list(vocabSet)\n",
    "    return vocabList\n",
    "\n",
    "def setOfWords2Vec(vocabList,words):\n",
    "    wordVec=[0]*len(vocabList)\n",
    "    for word in words:\n",
    "        if word in vocabList:\n",
    "            wordVec[vocabList.index(word)]=1        \n",
    "    return wordVec\n",
    "\n",
    "def bagOfWords2Vec(vocabList,words):\n",
    "    wordVec=[0]*len(vocabList)\n",
    "    for word in words:\n",
    "        if word in vocabList:\n",
    "            wordVec[vocabList.index(word)]+=1        \n",
    "    return wordVec\n",
    "\n",
    "def trainNB(trainMat,classList):\n",
    "    numWords=len(vocabList)\n",
    "    pSpam=(sum(classList)+1)/(len(classList)+2)\n",
    "    p1Num=np.ones(numWords)\n",
    "    p0Num=np.ones(numWords)\n",
    "    p1Denom=0\n",
    "    p0Denom=0\n",
    "    for i in range(len(classList)):\n",
    "        if classList[i]==1:\n",
    "            p1Num+=trainMat[i]\n",
    "            p1Denom+=sum(trainMat[i])\n",
    "        else:\n",
    "            p0Num+=trainMat[i]\n",
    "            p0Denom+=sum(trainMat[i])\n",
    "    p1Denom+=numWords\n",
    "    p0Denom+=numWords\n",
    "    p1Vec=np.log(p1Num/p1Denom)     \n",
    "    p0Vec=np.log(p0Num/p0Denom)\n",
    "    return p1Vec,p0Vec,pSpam\n",
    "\n",
    "def classifyNB(newWordVec,classList):\n",
    "    p1Vec,p0Vec,pSpam=trainNB(trainMat,classList)\n",
    "    p1=sum(newWordVec*p1Vec)+np.log(pSpam)\n",
    "    p0=sum(newWordVec*p0Vec)+np.log(1-pSpam)\n",
    "    print(p1,p0)\n",
    "    if p1>p0:\n",
    "        return ('粗鲁')\n",
    "    else:\n",
    "        return ('文明')\n",
    "\n",
    "def textParse1(line):\n",
    "    line=line.strip()\n",
    "    line=re.sub(r'[a-zA-Z.【】0-9、。，/！…~\\*\\n]','',line)\n",
    "    line=jieba.lcut(line,cut_all=True)\n",
    "    return [w for w in line if len(w)>1]\n",
    "\n",
    "wordList=[];classList=[]\n",
    "for i in range(127):  \n",
    "    wordList_s=textParse1(open('G:\\\\代码\\\\新建文件夹\\\\朴素贝叶斯垃圾邮件分类\\\\垃圾邮件\\\\%d.txt'%i,encoding='utf8').read())\n",
    "    wordList.append(wordList_s)\n",
    "    classList.append(1)\n",
    "for i in range(29):   \n",
    "    wordList_h=textParse1(open('G:\\\\代码\\\\新建文件夹\\\\朴素贝叶斯垃圾邮件分类\\\\正常邮件\\\\%d.txt'%i,encoding='utf8').read())\n",
    "    wordList.append(wordList_h)\n",
    "    classList.append(0)\n",
    "    \n",
    "vocabList=creatVocabList(wordList)\n",
    "trainMat=[]\n",
    "for words in wordList:\n",
    "    trainMat.append(setOfWords2Vec(vocabList,words))\n",
    "\n",
    "def predict(path):\n",
    "    testWords=textParse1(open(path,encoding='utf8').read())\n",
    "    newWordsVec=setOfWords2Vec(vocabList,testWords)\n",
    "    p1Vec,p0Vec,pSpam=trainNB(trainMat,classList)\n",
    "    return classifyNB(newWordsVec,classList)\n",
    "    \n",
    "path='G:\\\\代码\\\\新建文件夹\\\\朴素贝叶斯垃圾邮件分类\\\\垃圾邮件\\\\7.txt'\n",
    "predict(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn实现中文垃圾邮件处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def creatVocabList(wordList):\n",
    "    vocabSet=set([])\n",
    "    for document in wordList:\n",
    "        vocabSet=vocabSet|set(document)\n",
    "    vocabList=list(vocabSet)\n",
    "    return vocabList\n",
    "\n",
    "def setOfWords2Vec(vocabList,words):\n",
    "    wordVec=[0]*len(vocabList)\n",
    "    for word in words:\n",
    "        if word in vocabList:\n",
    "            wordVec[vocabList.index(word)]=1        \n",
    "    return wordVec\n",
    "\n",
    "def bagOfWords2Vec(vocabList,words):\n",
    "    wordVec=[0]*len(vocabList)\n",
    "    for word in words:\n",
    "        if word in vocabList:\n",
    "            wordVec[vocabList.index(word)]+=1        \n",
    "    return wordVec\n",
    "\n",
    "\n",
    "def textParse1(line):\n",
    "    line=re.sub(r'[a-zA-Z.【】0-9、。，/！…~\\*\\n]','',line)\n",
    "    line=jieba.lcut(line,cut_all=True)\n",
    "    return [w for w in line if len(w)>1]\n",
    "\n",
    "wordList=[];classList=[]\n",
    "for i in range(127):  \n",
    "    wordList_s=textParse1(open('G:\\\\代码\\\\新建文件夹\\\\朴素贝叶斯垃圾邮件分类\\\\垃圾邮件\\\\%d.txt'%i,encoding='utf8').read())\n",
    "    wordList.append(wordList_s)\n",
    "    classList.append(1)\n",
    "for i in range(29):   \n",
    "    wordList_h=textParse1(open('G:\\\\代码\\\\新建文件夹\\\\朴素贝叶斯垃圾邮件分类\\\\正常邮件\\\\%d.txt'%i,encoding='utf8').read())\n",
    "    wordList.append(wordList_h)\n",
    "    classList.append(0)\n",
    "    \n",
    "vocabList=creatVocabList(wordList)\n",
    "trainMat=[]\n",
    "for words in wordList:\n",
    "    trainMat.append(setOfWords2Vec(vocabList,words))\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
    "\n",
    "gn=GaussianNB()\n",
    "gn=gn.fit(trainMat,classList)\n",
    "testWords=textParse1(open('G:\\\\代码\\\\新建文件夹\\\\朴素贝叶斯垃圾邮件分类\\\\正常邮件\\\\1.txt',encoding='utf8').read())\n",
    "newWordVec=setOfWords2Vec(vocabList,testWords)\n",
    "gn.predict(np.array(newWordVec).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn.naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "iris=datasets.load_iris()\n",
    "gnb=GaussianNB()\n",
    "model=gnb.fit(iris.data,iris.target)\n",
    "y_predict = model.predict(iris.data)\n",
    "\n",
    "num = (iris.target==y_predict).sum()\n",
    "accuracy=num/iris.target.shape[0]\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn.feature_extraction.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词袋模型、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are', 'boyfriend', 'david', 'is', 'like', 'looks', 'much', 'my', 'name', 'nb', 'smart', 'stupid', 'very', 'you']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "wordList=['my name is David',\n",
    "             'you are stupid',\n",
    "             'my boyfriend is NB',\n",
    "             'you looks very smart I like you very much']\n",
    "classList=[0,1,1,0]\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv_fit=cv.fit_transform(wordList)\n",
    "print(cv.get_feature_names())\n",
    "# print(cv_fit.toarray())\n",
    "result2=cv_fit.toarray()\n",
    "#print(result2)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb=GaussianNB()\n",
    "model=gnb.fit(np.array(result2),np.array(classList))\n",
    "testWords=['you are stupid']\n",
    "resu = cv.transform(testWords)\n",
    "resu=resu.toarray()\n",
    "model.predict(resu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "wordList=['my name is David',\n",
    "             'you are stupid',\n",
    "             'my boyfriend is NB',\n",
    "             'you looks very smart I like you very much']\n",
    "classList=[0,1,1,0]\n",
    "\n",
    "tfidf2 = TfidfVectorizer()\n",
    "result = tfidf2.fit_transform(wordList)\n",
    "#print(tfidf2.get_feature_names())\n",
    "# print(result.shape)\n",
    "result=result.toarray()\n",
    "#print(np.round((result),2))\n",
    "#result=result.toarray()\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb=GaussianNB()\n",
    "model=gnb.fit(np.array(result),np.array(classList))\n",
    "testWords=['I like you']\n",
    "res = tfidf2.transform(testWords)\n",
    "res=res.toarray()\n",
    "model.predict(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "import re\n",
    "import numpy as np\n",
    "   \n",
    "def textParse1(lines):\n",
    "    lines=re.sub(r'[a-zA-Z.【】0-9、。，@（）+\"\"“”,:<>()：|？?\\[\\]\\-/！…~\\*\\n]','',lines)\n",
    "    words=jieba.lcut(lines)\n",
    "    new=[[w for w in words if len(w)>1]]\n",
    "    document=[\" \".join(sent) for sent in new]\n",
    "    return document\n",
    "\n",
    "wordList=[];classList=[]\n",
    "for i in range(127):  \n",
    "    wordList_s=textParse1(open('G:\\\\代码\\\\新建文件夹\\\\朴素贝叶斯垃圾邮件分类\\\\垃圾邮件\\\\%d.txt'%i,encoding='utf8').read())\n",
    "    wordList.append(','.join(wordList_s))\n",
    "    classList.append(1)\n",
    "for i in range(29):   \n",
    "    wordList_h=textParse1(open('G:\\\\代码\\\\新建文件夹\\\\朴素贝叶斯垃圾邮件分类\\\\正常邮件\\\\%d.txt'%i,encoding='utf8').read())\n",
    "    wordList.append(','.join(wordList_h))\n",
    "    classList.append(0)\n",
    "      \n",
    "tfidf1 = TfidfVectorizer()\n",
    "result1 = tfidf1.fit_transform(wordList)\n",
    "# print(tfidf2.get_feature_names())\n",
    "# print(result.shape)\n",
    "# print(result)\n",
    "result1=result1.toarray()\n",
    "#print(result1)\n",
    "\n",
    "gn=GaussianNB()\n",
    "gn=gn.fit(np.array(result1),np.array(classList))\n",
    "testWords=textParse1(open('G:\\\\代码\\\\新建文件夹\\\\朴素贝叶斯垃圾邮件分类\\\\垃圾邮件\\\\1.txt',encoding='utf8').read())\n",
    "res1 = tfidf1.transform(testWords)\n",
    "res1=res1.toarray()\n",
    "gn.predict(res1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
