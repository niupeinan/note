{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>朴素贝叶斯代码</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原理出发"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.载入训练数据集\n",
    "##### 2.创建词集（去重）\n",
    "##### 3.把训练集中文本转成向量形式（词集模型）\n",
    "##### 4.训练：计算先验概率和先验条件概率（即频率），为防止下溢，取对数\n",
    "##### 5.分类\n",
    "##### 6.测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    wordList=[['my','name','is','David'],\n",
    "                 ['you','are','stupid'],\n",
    "                 ['my','boyfriend','is','NB'],\n",
    "                 ['you','looks','very','smart','I','like','you','very','much']]\n",
    "    classList=[0,1,1,0]\n",
    "    return wordList,classList\n",
    "\n",
    "def creatVocabList(wordList):\n",
    "    vocabSet=set([])\n",
    "    for document in wordList:\n",
    "        vocabSet=vocabSet|set(document)\n",
    "    vocabList=list(vocabSet)\n",
    "    return vocabList\n",
    "\n",
    "def setOfWords2Vec(vocabList,words):\n",
    "    wordVec=[0]*len(vocabList)\n",
    "    for word in words:\n",
    "        if word in vocabList:\n",
    "            wordVec[vocabList.index(word)]=1        \n",
    "    return wordVec\n",
    "\n",
    "def bagOfWords2Vec(vocabList,words):\n",
    "    wordVec=[0]*len(vocabList)\n",
    "    for word in words:\n",
    "        if word in vocabList:\n",
    "            wordVec[vocabList.index(word)]+=1        \n",
    "    return wordVec\n",
    "\n",
    "wordList,classList=loadData()\n",
    "vocabList=creatVocabList(wordList)\n",
    "trainMat=[]\n",
    "for words in wordList:\n",
    "    trainMat.append(bagOfWords2Vec(vocabList,words))\n",
    "\n",
    "def trainNB(trainMat,classList):\n",
    "    numWords=len(vocabList)\n",
    "    pSpam=(sum(classList)+1)/(len(classList)+2)\n",
    "    p1Num=np.ones(numWords)\n",
    "    p0Num=np.ones(numWords)\n",
    "    p1Denom=0\n",
    "    p0Denom=0\n",
    "    for i in range(len(classList)):\n",
    "        if classList[i]==1:\n",
    "            p1Num+=trainMat[i]\n",
    "            p1Denom+=sum(trainMat[i])\n",
    "        else:\n",
    "            p0Num+=trainMat[i]\n",
    "            p0Denom+=sum(trainMat[i])\n",
    "    p1Denom+=numWords\n",
    "    p0Denom+=numWords\n",
    "    p1Vec=np.log(p1Num/p1Denom)     \n",
    "    p0Vec=np.log(p0Num/p0Denom)\n",
    "    return p1Vec,p0Vec,pSpam\n",
    "\n",
    "def classifyNB(newWordVec,classList):\n",
    "    p1Vec,p0Vec,pSpam=trainNB(trainMat,classList)\n",
    "    p1=sum(newWordVec*p1Vec)+np.log(pSpam)\n",
    "    p0=sum(newWordVec*p0Vec)+np.log(1-pSpam)\n",
    "    print(p1,p0)\n",
    "    if p1>p0:\n",
    "        return ('粗鲁')\n",
    "    else:\n",
    "        return ('文明')\n",
    "\n",
    "testWords=['NB']\n",
    "newWordVec=setOfWords2Vec(vocabList,testWords)\n",
    "ClassifyNB(newWordVec,classList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 垃圾邮件分类:英文版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.495541943884256 -8.350429973538136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'粗鲁'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "def creatVocabList(wordList):\n",
    "    vocabSet=set([])\n",
    "    for document in wordList:\n",
    "        vocabSet=vocabSet|set(document)\n",
    "    vocabList=list(vocabSet)\n",
    "    return vocabList\n",
    "\n",
    "def setOfWords2Vec(vocabList,words):\n",
    "    wordVec=[0]*len(vocabList)\n",
    "    for word in words:\n",
    "        if word in vocabList:\n",
    "            wordVec[vocabList.index(word)]=1        \n",
    "    return wordVec\n",
    "\n",
    "def bagOfWords2Vec(vocabList,words):\n",
    "    wordVec=[0]*len(vocabList)\n",
    "    for word in words:\n",
    "        if word in vocabList:\n",
    "            wordVec[vocabList.index(word)]+=1        \n",
    "    return wordVec\n",
    "\n",
    "def trainNB(trainMat,classList):\n",
    "    numWords=len(vocabList)\n",
    "    pSpam=(sum(classList)+1)/(len(classList)+2)\n",
    "    p1Num=np.ones(numWords)\n",
    "    p0Num=np.ones(numWords)\n",
    "    p1Denom=0\n",
    "    p0Denom=0\n",
    "    for i in range(len(classList)):\n",
    "        if classList[i]==1:\n",
    "            p1Num+=trainMat[i]\n",
    "            p1Denom+=sum(trainMat[i])\n",
    "        else:\n",
    "            p0Num+=trainMat[i]\n",
    "            p0Denom+=sum(trainMat[i])\n",
    "    p1Denom+=numWords\n",
    "    p0Denom+=numWords\n",
    "    p1Vec=np.log(p1Num/p1Denom)     \n",
    "    p0Vec=np.log(p0Num/p0Denom)\n",
    "    return p1Vec,p0Vec,pSpam\n",
    "\n",
    "def classifyNB(newWordVec,classList):\n",
    "    p1Vec,p0Vec,pSpam=trainNB(trainMat,classList)\n",
    "    p1=sum(newWordVec*p1Vec)+np.log(pSpam)\n",
    "    p0=sum(newWordVec*p0Vec)+np.log(1-pSpam)\n",
    "    print(p1,p0)\n",
    "    if p1>p0:\n",
    "        return ('粗鲁')\n",
    "    else:\n",
    "        return ('文明')\n",
    "\n",
    "def textParse(bigString):\n",
    "    line=re.split('\\W*',bigString)\n",
    "    return [tokens.lower() for tokens in line if len(tokens)>2]\n",
    "\n",
    "for i in range(1,26):\n",
    "    wordList=[];classList=[]\n",
    "    wordList_s=textParse(open('D:\\\\Downloads\\\\Naive_Bayes-master\\\\email\\\\spam\\\\%d.txt'%i).read())\n",
    "    wordList.append(wordList_s)\n",
    "    classList.append(1)\n",
    "    \n",
    "    wordList_h=textParse(open('D:\\\\Downloads\\\\Naive_Bayes-master\\\\email\\\\health\\\\%d.txt'%i).read())\n",
    "    wordList.append(wordList_h)\n",
    "    classList.append(0)\n",
    "    \n",
    "vocabList=creatVocabList(wordList)\n",
    "trainMat=[]\n",
    "for words in wordList:\n",
    "    trainMat.append(setOfWords2Vec(vocabList,words))\n",
    "\n",
    "def predict(path):\n",
    "    testWords=textParse(open(path).read())\n",
    "    newWordsVec=setOfWords2Vec(vocabList,testWords)\n",
    "    p1Vec,p0Vec,pSpam=trainNB(trainMat,classList)\n",
    "    return classifyNB(newWordsVec,classList)\n",
    "    \n",
    "path='D:\\\\Downloads\\\\Naive_Bayes-master\\\\email\\\\spam\\\\6.txt'\n",
    "predict(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改进版：预测新邮件：中文版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 3.417 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-361.7496110786985 -447.27585992956904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'粗鲁'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def creatVocabList(wordList):\n",
    "    vocabSet=set([])\n",
    "    for document in wordList:\n",
    "        vocabSet=vocabSet|set(document)\n",
    "    vocabList=list(vocabSet)\n",
    "    return vocabList\n",
    "\n",
    "def setOfWords2Vec(vocabList,words):\n",
    "    wordVec=[0]*len(vocabList)\n",
    "    for word in words:\n",
    "        if word in vocabList:\n",
    "            wordVec[vocabList.index(word)]=1        \n",
    "    return wordVec\n",
    "\n",
    "def bagOfWords2Vec(vocabList,words):\n",
    "    wordVec=[0]*len(vocabList)\n",
    "    for word in words:\n",
    "        if word in vocabList:\n",
    "            wordVec[vocabList.index(word)]+=1        \n",
    "    return wordVec\n",
    "\n",
    "def trainNB(trainMat,classList):\n",
    "    numWords=len(vocabList)\n",
    "    pSpam=(sum(classList)+1)/(len(classList)+2)\n",
    "    p1Num=np.ones(numWords)\n",
    "    p0Num=np.ones(numWords)\n",
    "    p1Denom=0\n",
    "    p0Denom=0\n",
    "    for i in range(len(classList)):\n",
    "        if classList[i]==1:\n",
    "            p1Num+=trainMat[i]\n",
    "            p1Denom+=sum(trainMat[i])\n",
    "        else:\n",
    "            p0Num+=trainMat[i]\n",
    "            p0Denom+=sum(trainMat[i])\n",
    "    p1Denom+=numWords\n",
    "    p0Denom+=numWords\n",
    "    p1Vec=np.log(p1Num/p1Denom)     \n",
    "    p0Vec=np.log(p0Num/p0Denom)\n",
    "    return p1Vec,p0Vec,pSpam\n",
    "\n",
    "def classifyNB(newWordVec,classList):\n",
    "    p1Vec,p0Vec,pSpam=trainNB(trainMat,classList)\n",
    "    p1=sum(newWordVec*p1Vec)+np.log(pSpam)\n",
    "    p0=sum(newWordVec*p0Vec)+np.log(1-pSpam)\n",
    "    print(p1,p0)\n",
    "    if p1>p0:\n",
    "        return ('粗鲁')\n",
    "    else:\n",
    "        return ('文明')\n",
    "\n",
    "def textParse1(line):\n",
    "    line=line.strip()\n",
    "    line=re.sub(r'[a-zA-Z.【】0-9、。，/！…~\\*\\n]','',line)\n",
    "    line=jieba.lcut(line,cut_all=True)\n",
    "    return [w for w in line if len(w)>1]\n",
    "\n",
    "wordList=[];classList=[]\n",
    "for i in range(127):  \n",
    "    wordList_s=textParse1(open('G:\\\\代码\\\\新建文件夹\\\\朴素贝叶斯垃圾邮件分类\\\\垃圾邮件\\\\%d.txt'%i,encoding='utf8').read())\n",
    "    wordList.append(wordList_s)\n",
    "    classList.append(1)\n",
    "for i in range(29):   \n",
    "    wordList_h=textParse1(open('G:\\\\代码\\\\新建文件夹\\\\朴素贝叶斯垃圾邮件分类\\\\正常邮件\\\\%d.txt'%i,encoding='utf8').read())\n",
    "    wordList.append(wordList_h)\n",
    "    classList.append(0)\n",
    "    \n",
    "vocabList=creatVocabList(wordList)\n",
    "trainMat=[]\n",
    "for words in wordList:\n",
    "    trainMat.append(setOfWords2Vec(vocabList,words))\n",
    "\n",
    "def predict(path):\n",
    "    testWords=textParse1(open(path,encoding='utf8').read())\n",
    "    newWordsVec=setOfWords2Vec(vocabList,testWords)\n",
    "    p1Vec,p0Vec,pSpam=trainNB(trainMat,classList)\n",
    "    return classifyNB(newWordsVec,classList)\n",
    "    \n",
    "path='G:\\\\代码\\\\新建文件夹\\\\朴素贝叶斯垃圾邮件分类\\\\垃圾邮件\\\\7.txt'\n",
    "predict(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  sklearn熟悉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "iris=datasets.load_iris()\n",
    "gnb=GaussianNB()\n",
    "model=gnb.fit(iris.data,iris.target)\n",
    "y_predict = model.predict(iris.data)\n",
    "\n",
    "num = (iris.target==y_predict).sum()\n",
    "accuracy=num/iris.target.shape[0]\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实现最初案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loadData():\n",
    "    wordList=[['my','name','is','David'],\n",
    "                 ['you','are','stupid'],\n",
    "                 ['my','boyfriend','is','NB'],\n",
    "                 ['you','looks','very','smart','I','like','you','very','much']]\n",
    "    classList=[0,1,1,0]\n",
    "    return wordList,classList\n",
    "\n",
    "def creatVocabList(wordList):\n",
    "    vocabSet=set([])\n",
    "    for document in wordList:\n",
    "        vocabSet=vocabSet|set(document)\n",
    "    vocabList=list(vocabSet)\n",
    "    return vocabList\n",
    "\n",
    "def setOfWords2Vec(vocabList,words):\n",
    "    wordVec=[0]*len(vocabList)\n",
    "    for word in words:\n",
    "        if word in vocabList:\n",
    "            wordVec[vocabList.index(word)]=1        \n",
    "    return wordVec\n",
    "\n",
    "def bagOfWords2Vec(vocabList,words):\n",
    "    wordVec=[0]*len(vocabList)\n",
    "    for word in words:\n",
    "        if word in vocabList:\n",
    "            wordVec[vocabList.index(word)]+=1        \n",
    "    return wordVec\n",
    "\n",
    "wordList,classList=loadData()\n",
    "vocabList=creatVocabList(wordList)\n",
    "trainMat=[]\n",
    "for words in wordList:\n",
    "    trainMat.append(bagOfWords2Vec(vocabList,words))\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb=GaussianNB()\n",
    "model=gnb.fit(np.array(trainMat),np.array(classList))\n",
    "testWords=['stupid']\n",
    "newWordVec=np.array(setOfWords2Vec(vocabList,testWords)).reshape(1,-1)\n",
    "model.predict(newWordVec)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
